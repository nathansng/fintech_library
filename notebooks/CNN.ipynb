{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a8ebf572-ba58-4b8e-bf64-267ffa8e3f98",
   "metadata": {},
   "source": [
    "# CNN Implementation in TreNet\n",
    "\n",
    "### Nathan Ng"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad5854b8-28ac-41f9-838b-c080804b7ecc",
   "metadata": {},
   "source": [
    "### CNN Implementation\n",
    "\n",
    "- CNN of *k* filters of window size *w* is applied to raw time series data\n",
    "- ReLU activation function is applied on CNN outputs\n",
    "- Results are pooled in max pooling layer \n",
    "- Finally, pooled outputs are concatenated to form fully connected layer \n",
    "\n",
    "- Note: Dropout layer is skipped because optimal TreNet hyperparameters found for NYSE data had dropout of 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa3c60e8-16e4-42be-a57b-d85146e2e63e",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a2fc4304-b3f7-42d2-b298-79064b17410a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bd81bac9-f4f5-4b97-93c9-0494e9dc6708",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use GPU if available for PyTorch\n",
    "if torch.cuda.is_available():\n",
    "    dev = \"cuda:0\"\n",
    "else:\n",
    "    dev = \"cpu\"\n",
    "\n",
    "device = torch.device(dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "df258e20-ad96-4687-9225-fcd11b26b71d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in subset of NYSE data \n",
    "df = pd.read_csv(\"../data/raw/indexProcessed.csv\")\n",
    "NYSE_data = df[df[\"Index\"] == \"NYA\"].loc[:, \"Open\"].reset_index(drop=True)\n",
    "data_cpu = torch.tensor(NYSE_data, dtype=torch.float)\n",
    "sub_cpu = torch.tensor(NYSE_data[:1000], dtype=torch.float) # First 1000 datapoints of NYSE \n",
    "\n",
    "# Move data to GPU \n",
    "data = data_cpu.to(device)\n",
    "sub = sub_cpu.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45946ecf-ecd0-42da-8839-1a2526b6d35d",
   "metadata": {},
   "source": [
    "## CNN Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "21827809-c77f-49bf-9bb1-b0acf2acd435",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TreNetCNN(nn.Module):\n",
    "    def __init__(self, num_data, layers=None, num_filters=None, dropout=None, conv_size=3, pooling_size=3, output_size=2):\n",
    "        \"\"\"\n",
    "        layers (int): Number of cnn stacks to create \n",
    "        num_filters (list(int)): Number of CNN filters corresponding to same index stacks\n",
    "        dropout (list(float)): Probability of dropout corresponing to same index stack\n",
    "        convsize (int, list(int)): Size of filter sizes \n",
    "        \"\"\"\n",
    "        super(TreNetCNN, self).__init__()\n",
    "        self.num_data = num_data\n",
    "        self.layers = layers\n",
    "        self.num_filters = num_filters\n",
    "        self.dropout = dropout\n",
    "        self.conv_size = conv_size\n",
    "        self.pooling_size = pooling_size\n",
    "        self.output_size = output_size\n",
    "        self.cnn_stack = self.create_cnn_stack()\n",
    "    \n",
    "    def create_cnn_stack(self):\n",
    "        # Initialize default stack settings\n",
    "        if not self.layers:\n",
    "            self.layers = 2\n",
    "        if not self.num_filters:\n",
    "            self.num_filters = [128] * self.layers\n",
    "        if not self.dropout:\n",
    "            self.dropout = [0.0] * self.layers\n",
    "        if type(self.conv_size) == int:\n",
    "            self.conv_size = [self.conv_size] * self.layers\n",
    "        \n",
    "        # Create cnn stacks \n",
    "        cnn_stacks = []\n",
    "        num_channels = 1\n",
    "        updated_data = self.num_data\n",
    "        for i in range(self.layers):\n",
    "            cnn_stack = nn.Sequential(\n",
    "                nn.Conv1d(in_channels=num_channels, out_channels=self.num_filters[i], kernel_size=self.conv_size[i]),\n",
    "                nn.ReLU(),\n",
    "                nn.MaxPool1d(kernel_size=self.pooling_size, stride=1),\n",
    "                nn.Dropout(p=self.dropout[i])\n",
    "            )\n",
    "            num_channels = self.num_filters[i]\n",
    "            cnn_stacks.append(cnn_stack)\n",
    "            \n",
    "            # Keep track of current size of data \n",
    "            updated_data = updated_data - self.conv_size[i] + 1 - self.pooling_size + 1\n",
    "            new_data_size = self.num_filters[i] * updated_data\n",
    "            \n",
    "        # Add fully connected layer at end to output trend duration and slope\n",
    "        output_layer = nn.Sequential(\n",
    "            nn.Flatten(), \n",
    "            nn.Linear(new_data_size, self.output_size)\n",
    "        )\n",
    "        cnn_stacks.append(output_layer)\n",
    "            \n",
    "        # Combine cnn stacks \n",
    "        return nn.Sequential(*cnn_stacks)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = torch.reshape(x, (1, 1, -1))\n",
    "        output = self.cnn_stack(x)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f379c53-a6e5-48e3-9ad3-e3dde7826f2a",
   "metadata": {},
   "source": [
    "## Setup Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e125c24e-4923-4eaa-9f8f-dc32690dbb3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Extracts m sequential data to use to predict n next data \n",
    "\"\"\"\n",
    "def extract_data(data, num_input, num_output):\n",
    "    num_rows = data.shape[0] - num_input - num_output\n",
    "    input_data = torch.zeros(num_rows, num_input)\n",
    "    output_data = torch.zeros(num_rows, num_output)\n",
    "    \n",
    "    for i in range(num_rows):\n",
    "        input_data[i] = (data[i:i+num_input])\n",
    "        output_data[i] = (data[i+num_input:i+num_input+num_output])\n",
    "    return input_data, output_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4b5e4c62-0532-4ad6-a8ec-954598b982d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Separates data into train, validation, and test sets\n",
    "props: (train, valid)\n",
    "\"\"\"\n",
    "def train_valid_test_split(X, y, props=None):\n",
    "    if not props: \n",
    "        props = (0.5, 0.25)\n",
    "    elif len(props) != 3: \n",
    "        print(\"Wrong number of parameters\")\n",
    "        return None\n",
    "    \n",
    "    train_size = int(X.shape[0] * props[0])\n",
    "    valid_size = int(X.shape[0] * props[1])\n",
    "    \n",
    "    X_train = X[:train_size].to(device)\n",
    "    y_train = y[:train_size].to(device)\n",
    "    \n",
    "    X_valid = X[train_size: (train_size + valid_size)].to(device)\n",
    "    y_valid = y[train_size: (train_size + valid_size)].to(device)\n",
    "    \n",
    "    X_test = X[(train_size + valid_size):].to(device)\n",
    "    y_test = y[(train_size + valid_size):].to(device)\n",
    "    \n",
    "    return X_train, y_train, X_valid, y_valid, X_test, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba218bc7-b591-4eeb-ab15-8583afe70892",
   "metadata": {},
   "source": [
    "## TreNet CNN Stack Model and Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "48641870-02c5-4fb5-b7cf-6d0726ce15d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Create training loop\n",
    "\"\"\"\n",
    "def train_loop(X, y, model, loss_fn, optimizer):\n",
    "    for i in range(X.shape[0]):\n",
    "        # Compute prediction and loss \n",
    "        pred = model(X[i])\n",
    "        loss = loss_fn(pred, y[i])\n",
    "        \n",
    "        # Perform backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dbea557f-93b6-47f8-a822-27000ea5fe14",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Create test loop\n",
    "\"\"\"\n",
    "def test_loop(X, y, model, loss_fn):\n",
    "    test_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i in range(X.shape[0]):\n",
    "            pred = model(X[i])\n",
    "            test_loss += loss_fn(pred, y[i]).item()\n",
    "            \n",
    "    test_loss /= X.shape[0]\n",
    "\n",
    "    return test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ed14de6c-5bf6-46f1-8b9e-fc82d10f0f8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "RMSE Loss Function\n",
    "\"\"\"\n",
    "def rmse(output, target):\n",
    "    loss = torch.mean((output - target)**2)\n",
    "    return torch.sqrt(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32fe47df-1c6a-4cc4-9a8a-b9138aa9ce15",
   "metadata": {},
   "source": [
    "## Train and Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "514be90d-1cbe-4347-a519-a074f46b9f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate train, validation, and test sets\n",
    "num_outputs = 1\n",
    "X, y = extract_data(data, 100, num_outputs)\n",
    "train_X, train_y, valid_X, valid_y, test_X, test_y = train_valid_test_split(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c55c25d5-2586-41dc-8ced-a410516f6595",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set hyperparameters for training \n",
    "learning_rate = 0.000001\n",
    "batch_size = 64\n",
    "epochs = 100\n",
    "\n",
    "# Create model\n",
    "model = TreNetCNN(100, layers = 2, num_filters = [32, 32], dropout=[0.1, 0.1], conv_size=[2, 4], output_size=num_outputs).to(device)\n",
    "\n",
    "# Initialize loss function \n",
    "loss_fn = rmse\n",
    "\n",
    "# Initialize optimizer \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "88c71b49-f23f-4e9e-aeb1-eed2104d4ed3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/torch/nn/functional.py:652: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)\n",
      "  return torch.max_pool1d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:\n",
      "------------------\n",
      "Train Loss: 61.48894962595667\n",
      "Validation Loss: 330.3544305351145\n",
      "\n",
      "Epoch 5:\n",
      "------------------\n",
      "Train Loss: 60.10750168443983\n",
      "Validation Loss: 315.8661121102779\n",
      "\n",
      "Epoch 10:\n",
      "------------------\n",
      "Train Loss: 56.04740053447154\n",
      "Validation Loss: 290.7331832184746\n",
      "\n",
      "Epoch 15:\n",
      "------------------\n",
      "Train Loss: 53.69540679019155\n",
      "Validation Loss: 281.98063437906313\n",
      "\n",
      "Epoch 20:\n",
      "------------------\n",
      "Train Loss: 51.20694917477436\n",
      "Validation Loss: 271.66515693565356\n",
      "\n",
      "Epoch 25:\n",
      "------------------\n",
      "Train Loss: 50.9729223211394\n",
      "Validation Loss: 263.8370854053426\n",
      "\n",
      "Epoch 30:\n",
      "------------------\n",
      "Train Loss: 49.074368327128546\n",
      "Validation Loss: 256.989931121447\n",
      "\n",
      "Epoch 35:\n",
      "------------------\n",
      "Train Loss: 47.92379934423045\n",
      "Validation Loss: 251.31341244119744\n",
      "\n",
      "Epoch 40:\n",
      "------------------\n",
      "Train Loss: 46.479538023377124\n",
      "Validation Loss: 248.19764146471397\n",
      "\n",
      "Epoch 45:\n",
      "------------------\n",
      "Train Loss: 45.65956372937835\n",
      "Validation Loss: 247.71206614778967\n",
      "\n",
      "Epoch 50:\n",
      "------------------\n",
      "Train Loss: 45.73364295722535\n",
      "Validation Loss: 236.3791642871009\n",
      "\n",
      "Epoch 55:\n",
      "------------------\n",
      "Train Loss: 44.24462034122699\n",
      "Validation Loss: 231.53009652195897\n",
      "\n",
      "Epoch 60:\n",
      "------------------\n",
      "Train Loss: 43.43104296007478\n",
      "Validation Loss: 233.80595454822486\n",
      "\n",
      "Epoch 65:\n",
      "------------------\n",
      "Train Loss: 42.907649590000325\n",
      "Validation Loss: 223.77658896679065\n",
      "\n",
      "Epoch 70:\n",
      "------------------\n",
      "Train Loss: 41.839146258254864\n",
      "Validation Loss: 220.28996661742\n",
      "\n",
      "Epoch 75:\n",
      "------------------\n",
      "Train Loss: 41.35961974013253\n",
      "Validation Loss: 219.8498266396995\n",
      "\n",
      "Epoch 80:\n",
      "------------------\n",
      "Train Loss: 40.92089694754746\n",
      "Validation Loss: 219.0514632354269\n",
      "\n",
      "Epoch 85:\n",
      "------------------\n",
      "Train Loss: 40.7699830406373\n",
      "Validation Loss: 217.36657048236148\n",
      "\n",
      "Epoch 90:\n",
      "------------------\n",
      "Train Loss: 40.15211368898565\n",
      "Validation Loss: 214.28183365198822\n",
      "\n",
      "Epoch 95:\n",
      "------------------\n",
      "Train Loss: 39.74540760040834\n",
      "Validation Loss: 209.85956729336715\n",
      "\n",
      "Epoch 99:\n",
      "------------------\n",
      "Train Loss: 38.71534638416556\n",
      "Validation Loss: 209.23063339149633\n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "train_score = []\n",
    "valid_score = []\n",
    "for i in range(epochs):\n",
    "    train_loop(train_X, train_y, model, loss_fn, optimizer)\n",
    "    train_score.append(test_loop(train_X, train_y, model, loss_fn))\n",
    "    valid_score.append(test_loop(valid_X, valid_y, model, loss_fn))\n",
    "    \n",
    "    if i % 5 == 0 or i == epochs-1: \n",
    "        print(f\"Epoch {i}:\\n------------------\")\n",
    "        print(f\"Train Loss: {train_score[-1]}\")\n",
    "        print(f\"Validation Loss: {valid_score[-1]}\")\n",
    "        print()\n",
    "    \n",
    "print(\"Done!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
