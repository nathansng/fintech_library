{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM Implementation in TreNet\n",
    "\n",
    "### Authors: Nathan Ng, Gao Mo, Richard Tang"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM Implementation\n",
    "\n",
    "- LSTM model that feeds into linear layer that matches number of outputs as CNN stack\n",
    "- Takes as input stock trend durations and slopes "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use GPU if available for PyTorch\n",
    "if torch.cuda.is_available():\n",
    "    dev = \"cuda:0\"\n",
    "else:\n",
    "    dev = \"cpu\"\n",
    "\n",
    "device = torch.device(dev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Scales data using Sklearn's MinMaxScaler\n",
    "Generalized to accept tensors \n",
    "\"\"\"\n",
    "class Scaler():\n",
    "    def __init__(self):\n",
    "        self.scaler = MinMaxScaler()\n",
    "        \n",
    "    def fit_transform(self, data):\n",
    "        # Check if data is tensor\n",
    "        if type(data) == torch.Tensor:\n",
    "            data = torch.Tensor.cpu(data).detach().numpy()\n",
    "            \n",
    "        # Check if data is dataframe \n",
    "        if type(data) == pd.DataFrame or type(data) == pd.Series:\n",
    "            data = data.values\n",
    "            \n",
    "        # Transform data \n",
    "        if len(data.shape) == 1: \n",
    "            scaled_data = self.scaler.fit_transform(data.reshape(-1, 1)).flatten()\n",
    "        else: \n",
    "            scaled_data = self.scaler.fit_transform(data)\n",
    "        \n",
    "        # Return tensor of scaled data\n",
    "        return torch.tensor(scaled_data, dtype=torch.float)\n",
    "    \n",
    "    def inverse_transform(self, data): \n",
    "        # Check if data is tensor\n",
    "        if type(data) == torch.Tensor:\n",
    "            data = torch.Tensor.cpu(data).detach().numpy()\n",
    "            \n",
    "        # Check if data is dataframe \n",
    "        if type(data) == pd.DataFrame or type(data) == pd.Series:\n",
    "            data = data.values\n",
    "        \n",
    "        inverse_data = self.scaler.inverse_transform(data)\n",
    "        \n",
    "        # Return tensor of inverse data\n",
    "        return torch.tensor(inverse_data, dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Extracts m sequential data to use to predict n next data \n",
    "\"\"\"\n",
    "def extract_data(data, num_input, num_output):\n",
    "    num_rows = data.shape[0] - num_input - num_output\n",
    "    input_data = torch.zeros(num_rows, num_input)\n",
    "    output_data = torch.zeros(num_rows, num_output)\n",
    "    \n",
    "    for i in range(num_rows):\n",
    "        input_data[i] = (data[i:i+num_input])\n",
    "        output_data[i] = (data[i+num_input:i+num_input+num_output])\n",
    "    return input_data, output_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Separates data into train, validation, and test sets\n",
    "props: (train, valid)\n",
    "\"\"\"\n",
    "def train_valid_test_split(X, y=None, props=None, device=None):\n",
    "    if not props:\n",
    "        props = (0.5, 0.25)\n",
    "    elif len(props) != 3:\n",
    "        print(\"Wrong number of parameters\")\n",
    "        return None\n",
    "\n",
    "    train_size = int(X.shape[0] * props[0])\n",
    "    valid_size = int(X.shape[0] * props[1])\n",
    "\n",
    "    X_train = X[:train_size].to(device)\n",
    "    X_valid = X[train_size: (train_size + valid_size)].to(device)\n",
    "    X_test = X[(train_size + valid_size):].to(device)\n",
    "\n",
    "    if y != None:\n",
    "        y_train = y[:train_size].to(device)\n",
    "        y_valid = y[train_size: (train_size + valid_size)].to(device)\n",
    "        y_test = y[(train_size + valid_size):].to(device)\n",
    "\n",
    "        return X_train, y_train, X_valid, y_valid, X_test, y_test\n",
    "\n",
    "    return X_train, X_valid, X_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TreNet LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_layers, output_dim, device=None):\n",
    "        super(LSTM, self).__init__()\n",
    "\n",
    "        # Initialize hidden dimenision and layers\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.input_dim = input_dim\n",
    "\n",
    "        # Initialize deep learning models\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, batch_first=True).to(device)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim).to(device)\n",
    "\n",
    "        self.device = device\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_dim).requires_grad_().to(self.device)\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_dim).requires_grad_().to(self.device)\n",
    "\n",
    "        # Reshape data if needed\n",
    "        if len(x.shape) != 3:\n",
    "            x = x.reshape(x.shape[0], -1, self.input_dim).to(self.device)\n",
    "\n",
    "        # Run data through model\n",
    "        out, (hn, cn) = self.lstm(x, (h0, c0))\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TreNet LSTM Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Create training loop\n",
    "\"\"\"\n",
    "def train_loop(n_epochs, X, y, model, loss_fn, optimizer, X_val=None, y_val=None, printout=False, record_loss=False):\n",
    "    train_losses = []\n",
    "    best_model = None\n",
    "    best_loss = float(\"inf\")\n",
    "    validation = False\n",
    "    \n",
    "    # Add data for validation loss\n",
    "    if X_val is not None and y_val is not None: \n",
    "        validation = True\n",
    "        val_losses = []\n",
    "    \n",
    "    for i in range(n_epochs):\n",
    "        # Compute prediction and loss\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Compute validation loss \n",
    "        if validation:\n",
    "            val_pred = model(X_val)\n",
    "            val_loss = loss_fn(val_pred, y_val)\n",
    "            \n",
    "        # Store best performing model\n",
    "        if validation: \n",
    "            if val_loss.item() < best_loss:\n",
    "                best_loss = val_loss.item()\n",
    "                best_model = model.state_dict()\n",
    "                torch.save(model.state_dict(), \"best_model\")\n",
    "        else:\n",
    "            if loss.item() < best_loss: \n",
    "                best_loss = loss.item()\n",
    "                best_model = model.state_dict()\n",
    "                torch.save(model.state_dict(), \"best_model\")\n",
    "                \n",
    "\n",
    "        # Perform backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Print loss per epoch\n",
    "        if printout and i % 100 == 0:\n",
    "            print(f\"Epoch {i}:\\n--------------\")\n",
    "            print(f\"Train Loss: {np.sqrt(loss.item())}\")\n",
    "            \n",
    "            if val_loss is not None:\n",
    "                print(f\"Validation Loss: {np.sqrt(val_loss.item())}\")\n",
    "\n",
    "            print()\n",
    "\n",
    "        # Record loss per epoch\n",
    "        if record_loss:\n",
    "            train_losses.append(np.sqrt(loss.item()))\n",
    "            if validation:\n",
    "                val_losses.append(np.sqrt(val_loss.item()))\n",
    "    \n",
    "    # Calculate final model losses \n",
    "    pred = model(X)\n",
    "    final_train_loss = loss_fn(pred, y)\n",
    "    \n",
    "    if validation:\n",
    "        val_pred = model(X_val)\n",
    "        final_val_loss = loss_fn(val_pred, y_val)\n",
    "    \n",
    "    # Update model to best performing model\n",
    "    model.load_state_dict(torch.load(\"best_model\"))\n",
    "    pred = model(X)\n",
    "    best_train_loss = loss_fn(pred, y)\n",
    "    \n",
    "    if validation:\n",
    "        val_pred = model(X_val)\n",
    "        best_val_loss = loss_fn(val_pred, y_val)\n",
    "\n",
    "    # Print final loss after training and best performing model \n",
    "    if printout:\n",
    "        print(f\"Final:\\n--------------\")\n",
    "        print(f\"Train Loss: {np.sqrt(final_train_loss.item())}\")\n",
    "        if validation:\n",
    "            print(f\"Validation Loss: {np.sqrt(final_val_loss.item())}\")\n",
    "        print()\n",
    "        \n",
    "        print(f\"Best Model:\\n--------------\")\n",
    "        print(f\"Train Loss: {np.sqrt(best_train_loss.item())}\")\n",
    "        if validation:\n",
    "            print(f\"Validation Loss: {np.sqrt(best_val_loss.item())}\")\n",
    "        print()\n",
    "\n",
    "    if record_loss: \n",
    "        if validation:\n",
    "            return train_losses, val_losses\n",
    "        else:\n",
    "            return train_losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test LSTM Model on Raw Stock Prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in NYSE data\n",
    "df = pd.read_csv(\"../data/raw/indexProcessed.csv\")\n",
    "NYSE_df = df[df[\"Index\"] == \"NYA\"].loc[:, \"Open\"].reset_index(drop=True)\n",
    "\n",
    "# Create subset of data \n",
    "NYSE_sub = NYSE_df[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale data \n",
    "scaler = Scaler()\n",
    "sub = scaler.fit_transform(NYSE_sub).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract samples and create train test sets\n",
    "X, y = extract_data(sub, 49, 1)\n",
    "X_train, y_train, X_valid, y_valid, X_test, y_test = train_valid_test_split(X, y, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set parameters for model and training \n",
    "input_dim = 1\n",
    "hidden_dim = 32\n",
    "num_layers = 1\n",
    "output_dim = 1\n",
    "\n",
    "learning_rate = 0.01\n",
    "num_epochs = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model, loss function, and optimizer\n",
    "model = LSTM(input_dim=input_dim, hidden_dim=hidden_dim, output_dim=output_dim, num_layers=num_layers, device=device)\n",
    "loss_fn = nn.MSELoss().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:\n",
      "--------------\n",
      "Train Loss: 0.4054621613248329\n",
      "Validation Loss: 0.701968831951709\n",
      "\n",
      "Epoch 100:\n",
      "--------------\n",
      "Train Loss: 0.02297416305335471\n",
      "Validation Loss: 0.024583437615773834\n",
      "\n",
      "Epoch 200:\n",
      "--------------\n",
      "Train Loss: 0.020452765399284804\n",
      "Validation Loss: 0.0238516288571945\n",
      "\n",
      "Epoch 300:\n",
      "--------------\n",
      "Train Loss: 0.01857707044474476\n",
      "Validation Loss: 0.020738318816857112\n",
      "\n",
      "Epoch 400:\n",
      "--------------\n",
      "Train Loss: 0.01743390605534811\n",
      "Validation Loss: 0.019369173355080656\n",
      "\n",
      "Epoch 500:\n",
      "--------------\n",
      "Train Loss: 0.016884484432144888\n",
      "Validation Loss: 0.018615638232430554\n",
      "\n",
      "Epoch 600:\n",
      "--------------\n",
      "Train Loss: 0.016428816551191466\n",
      "Validation Loss: 0.01845795030551108\n",
      "\n",
      "Epoch 700:\n",
      "--------------\n",
      "Train Loss: 0.015991786953692785\n",
      "Validation Loss: 0.018535335701193336\n",
      "\n",
      "Epoch 800:\n",
      "--------------\n",
      "Train Loss: 0.015531389422403733\n",
      "Validation Loss: 0.018665742118841786\n",
      "\n",
      "Epoch 900:\n",
      "--------------\n",
      "Train Loss: 0.015310397610494568\n",
      "Validation Loss: 0.018712245349431004\n",
      "\n",
      "Final:\n",
      "--------------\n",
      "Train Loss: 0.014903782026602592\n",
      "Validation Loss: 0.018676993892149992\n",
      "\n",
      "Best Model:\n",
      "--------------\n",
      "Train Loss: 0.016317783910008828\n",
      "Validation Loss: 0.015647359957211743\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Train model\n",
    "losses = train_loop(num_epochs, X_train, y_train, model, loss_fn, optimizer, printout=True, record_loss=True, X_val = X_valid, y_val = y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare predictions and actual values \n",
    "inverse_test_y = scaler.inverse_transform(y_test)\n",
    "\n",
    "pred_test_y = model(X_test)\n",
    "inverse_pred_test_y = scaler.inverse_transform(pred_test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Test Loss: \" + str(loss_fn(inverse_pred_test_y, inverse_test_y)**(1/2)))\n",
    "\n",
    "print(\"Actual values: \\n\" + str(inverse_test_y[:10]))\n",
    "\n",
    "print(\"Predicted values: \\n\" + str(inverse_pred_test_y[:10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save training loss as csv \n",
    "pd.DataFrame(training_loss).to_csv(\"../data/losses/lstm_loss.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving Model and Restoring Model State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = model.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2 = LSTM(input_dim=input_dim, hidden_dim=hidden_dim, output_dim=output_dim, num_layers=num_layers, device=device)\n",
    "# model_2.load_state_dict(best_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('lstm.weight_ih_l0',\n",
       "              tensor([[-0.2722],\n",
       "                      [-0.1052],\n",
       "                      [ 0.2542],\n",
       "                      [ 0.2783],\n",
       "                      [-0.3370],\n",
       "                      [ 0.2374],\n",
       "                      [ 0.0372],\n",
       "                      [ 0.2592],\n",
       "                      [ 0.3185],\n",
       "                      [-0.0600],\n",
       "                      [ 0.3775],\n",
       "                      [ 0.3695],\n",
       "                      [ 0.1134],\n",
       "                      [ 0.1399],\n",
       "                      [ 0.1551],\n",
       "                      [-0.4466],\n",
       "                      [ 0.3496],\n",
       "                      [-0.0355],\n",
       "                      [ 0.0471],\n",
       "                      [-0.1455],\n",
       "                      [-0.2234],\n",
       "                      [ 0.2945],\n",
       "                      [-0.3681],\n",
       "                      [ 0.2337],\n",
       "                      [ 0.5413],\n",
       "                      [ 0.2862],\n",
       "                      [ 0.2962],\n",
       "                      [ 0.3796],\n",
       "                      [ 0.4161],\n",
       "                      [-0.0495],\n",
       "                      [ 0.1954],\n",
       "                      [ 0.3669],\n",
       "                      [-0.4020],\n",
       "                      [-0.2772],\n",
       "                      [ 0.1813],\n",
       "                      [ 0.3479],\n",
       "                      [-0.2020],\n",
       "                      [ 0.2368],\n",
       "                      [ 0.0631],\n",
       "                      [ 0.1362],\n",
       "                      [-0.4557],\n",
       "                      [-0.0512],\n",
       "                      [ 0.3890],\n",
       "                      [ 0.3326],\n",
       "                      [ 0.0602],\n",
       "                      [ 0.0358],\n",
       "                      [ 0.2758],\n",
       "                      [-0.4289],\n",
       "                      [ 0.2217],\n",
       "                      [-0.2878],\n",
       "                      [-0.0419],\n",
       "                      [-0.0554],\n",
       "                      [-0.2809],\n",
       "                      [ 0.1701],\n",
       "                      [-0.2305],\n",
       "                      [ 0.0964],\n",
       "                      [ 0.5714],\n",
       "                      [ 0.1308],\n",
       "                      [ 0.2334],\n",
       "                      [ 0.0993],\n",
       "                      [ 0.2277],\n",
       "                      [-0.0480],\n",
       "                      [ 0.2364],\n",
       "                      [ 0.3824],\n",
       "                      [ 0.3413],\n",
       "                      [-0.0914],\n",
       "                      [-0.3860],\n",
       "                      [-0.2937],\n",
       "                      [ 0.1324],\n",
       "                      [-0.0303],\n",
       "                      [-0.3979],\n",
       "                      [-0.1407],\n",
       "                      [ 0.7012],\n",
       "                      [-0.0916],\n",
       "                      [ 0.3825],\n",
       "                      [ 0.2829],\n",
       "                      [-0.0129],\n",
       "                      [ 0.1455],\n",
       "                      [ 0.4410],\n",
       "                      [-0.3479],\n",
       "                      [-0.4592],\n",
       "                      [ 0.5433],\n",
       "                      [-0.3271],\n",
       "                      [ 0.1385],\n",
       "                      [ 0.0479],\n",
       "                      [ 0.2592],\n",
       "                      [ 0.3977],\n",
       "                      [ 0.1835],\n",
       "                      [-0.4866],\n",
       "                      [-0.2944],\n",
       "                      [-0.4310],\n",
       "                      [ 0.1696],\n",
       "                      [ 0.4499],\n",
       "                      [ 0.0157],\n",
       "                      [ 0.1967],\n",
       "                      [ 0.5477],\n",
       "                      [-0.5765],\n",
       "                      [-0.2157],\n",
       "                      [ 0.6563],\n",
       "                      [ 0.2140],\n",
       "                      [-0.2706],\n",
       "                      [ 0.2118],\n",
       "                      [ 0.2012],\n",
       "                      [ 0.2059],\n",
       "                      [ 0.0397],\n",
       "                      [-0.2747],\n",
       "                      [ 0.5376],\n",
       "                      [ 0.2267],\n",
       "                      [-0.1361],\n",
       "                      [ 0.1276],\n",
       "                      [ 0.3721],\n",
       "                      [-0.7948],\n",
       "                      [ 0.4757],\n",
       "                      [-0.0471],\n",
       "                      [ 0.2800],\n",
       "                      [-0.2608],\n",
       "                      [-0.1689],\n",
       "                      [ 0.1581],\n",
       "                      [-0.4575],\n",
       "                      [ 0.3035],\n",
       "                      [ 0.6457],\n",
       "                      [ 0.1982],\n",
       "                      [ 0.4356],\n",
       "                      [ 0.4188],\n",
       "                      [ 0.4404],\n",
       "                      [-0.0657],\n",
       "                      [ 0.4846],\n",
       "                      [ 0.2249]], device='cuda:0')),\n",
       "             ('lstm.weight_hh_l0',\n",
       "              tensor([[ 0.0381,  0.1592, -0.0486,  ...,  0.1288, -0.0044, -0.0459],\n",
       "                      [ 0.0161, -0.0535,  0.1150,  ...,  0.2018, -0.0131,  0.0574],\n",
       "                      [-0.7477, -0.0447, -0.1208,  ...,  0.3627,  0.2286,  0.0814],\n",
       "                      ...,\n",
       "                      [ 0.0532,  0.0857,  0.1807,  ...,  0.2227, -0.1880, -0.1527],\n",
       "                      [ 0.0949, -0.2162,  0.1692,  ..., -0.1970, -0.0992, -0.0398],\n",
       "                      [-0.2852, -0.0324,  0.0905,  ...,  0.0669, -0.0332, -0.0792]],\n",
       "                     device='cuda:0')),\n",
       "             ('lstm.bias_ih_l0',\n",
       "              tensor([-0.0468, -0.0963,  0.2296, -0.0010,  0.0908,  0.0039, -0.0597, -0.0603,\n",
       "                       0.6802, -0.0275,  0.1428,  0.2071, -0.2335, -0.1059,  0.1520,  0.1847,\n",
       "                       0.0464,  0.5414, -0.0727, -0.1628, -0.2465, -0.0026,  0.0168, -0.1624,\n",
       "                       0.1329,  0.0564, -0.0112, -0.0294,  0.1913, -0.1763, -0.0641,  0.2246,\n",
       "                       0.1120,  0.0214, -0.1635,  0.1763, -0.0483, -0.0744,  0.1454, -0.0780,\n",
       "                      -0.2633, -0.0964, -0.1513,  0.0554, -0.1678,  0.0715, -0.1625, -0.0037,\n",
       "                       0.0104, -0.3558,  0.1813,  0.0957, -0.1087,  0.0743,  0.0502, -0.0114,\n",
       "                       0.0745, -0.1276, -0.1806, -0.0028, -0.1693, -0.0161, -0.1414,  0.0961,\n",
       "                       0.0036,  0.1478, -0.0367, -0.1055, -0.1672,  0.1438, -0.0895, -0.1267,\n",
       "                      -0.0966,  0.1063,  0.1276,  0.1711,  0.0213,  0.1306,  0.0827,  0.1865,\n",
       "                       0.1199,  0.0846, -0.0829, -0.1538, -0.1362,  0.0904,  0.0142,  0.1922,\n",
       "                      -0.0790, -0.0440, -0.1623,  0.0999,  0.1061, -0.0397,  0.2004,  0.0234,\n",
       "                      -0.0494,  0.0266,  0.0672,  0.0568,  0.1060, -0.1857,  0.0453,  0.0695,\n",
       "                       0.4281, -0.0520,  0.0064, -0.0146, -0.2385,  0.0643,  0.0940,  0.0401,\n",
       "                       0.2582,  0.2957,  0.0495, -0.2382, -0.0431,  0.1551, -0.0108,  0.0897,\n",
       "                       0.1662,  0.2121,  0.2761, -0.0880,  0.1281, -0.1636,  0.0669,  0.1046],\n",
       "                     device='cuda:0')),\n",
       "             ('lstm.bias_hh_l0',\n",
       "              tensor([ 0.0954,  0.0915,  0.3198, -0.0645, -0.1381, -0.0688,  0.0720,  0.1004,\n",
       "                       0.4597, -0.0126,  0.1898, -0.0431,  0.0195, -0.1575,  0.1426, -0.0189,\n",
       "                       0.2027,  0.3340, -0.0340, -0.1331, -0.0944, -0.0845, -0.0345,  0.1746,\n",
       "                       0.0816, -0.0261,  0.1620,  0.1506,  0.1806, -0.0066, -0.0282,  0.0970,\n",
       "                      -0.0750, -0.1753, -0.2200, -0.0881,  0.0826, -0.0277, -0.1316,  0.0041,\n",
       "                      -0.2214, -0.2445,  0.0764,  0.1567, -0.0233,  0.0470, -0.0091, -0.1665,\n",
       "                      -0.0913, -0.2592,  0.0363, -0.2474, -0.0099, -0.0558, -0.2479,  0.0167,\n",
       "                       0.0925, -0.0762, -0.0540, -0.0525,  0.1171, -0.0013, -0.0510, -0.1572,\n",
       "                      -0.1019, -0.0966,  0.0539, -0.1726,  0.0980,  0.0980, -0.1269, -0.1842,\n",
       "                       0.0555, -0.0655, -0.0013,  0.0340,  0.1636,  0.1695,  0.0235,  0.1175,\n",
       "                      -0.1036, -0.1505, -0.0039,  0.0799,  0.0988,  0.0909, -0.1304, -0.0995,\n",
       "                      -0.1199, -0.1764,  0.1464,  0.1731, -0.0412, -0.0557,  0.1646, -0.0385,\n",
       "                       0.1424, -0.0736,  0.0107,  0.0383,  0.1048, -0.1033,  0.0732, -0.1062,\n",
       "                       0.6442, -0.2236,  0.1676, -0.1386, -0.0923, -0.1331, -0.0089, -0.0337,\n",
       "                       0.0255,  0.3823, -0.0445, -0.1639, -0.1740,  0.1364,  0.0171,  0.0267,\n",
       "                       0.0095,  0.0958,  0.2780,  0.0175,  0.1653, -0.1634,  0.0175,  0.1541],\n",
       "                     device='cuda:0')),\n",
       "             ('fc.weight',\n",
       "              tensor([[ 0.1585, -0.0314, -0.2133, -0.1879,  0.0380,  0.0833, -0.1557, -0.0758,\n",
       "                        0.6377,  0.0131,  0.2634,  0.1878,  0.0733,  0.1991,  0.1657, -0.2023,\n",
       "                       -0.3022,  0.3986, -0.2297,  0.0094,  0.0469,  0.2611,  0.2246,  0.1155,\n",
       "                       -0.1584, -0.2497, -0.2236,  0.1913,  0.3231,  0.0606,  0.2345,  0.2851]],\n",
       "                     device='cuda:0')),\n",
       "             ('fc.bias', tensor([-0.0492], device='cuda:0'))])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_2.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_2.load_state_dict(dict(model.state_dict()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.set()\n",
    "plt.plot(losses[0])\n",
    "plt.plot(losses[1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
