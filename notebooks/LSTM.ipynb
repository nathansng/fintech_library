{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM Implementation in TreNet\n",
    "\n",
    "### Authors: Nathan Ng, Gao Mo, Richard Tang"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM Implementation\n",
    "\n",
    "- LSTM model that feeds into linear layer that matches number of outputs as CNN stack\n",
    "- Takes as input stock trend durations and slopes "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use GPU if available for PyTorch\n",
    "if torch.cuda.is_available():\n",
    "    dev = \"cuda:0\"\n",
    "else:\n",
    "    dev = \"cpu\"\n",
    "\n",
    "device = torch.device(dev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Scales data using Sklearn's MinMaxScaler\n",
    "Generalized to accept tensors \n",
    "\"\"\"\n",
    "class Scaler():\n",
    "    def __init__(self):\n",
    "        self.scaler = MinMaxScaler()\n",
    "        \n",
    "    def fit_transform(self, data):\n",
    "        # Check if data is tensor\n",
    "        if type(data) == torch.Tensor:\n",
    "            data = torch.Tensor.cpu(data).detach().numpy()\n",
    "            \n",
    "        # Check if data is dataframe \n",
    "        if type(data) == pd.DataFrame or type(data) == pd.Series:\n",
    "            data = data.values\n",
    "            \n",
    "        # Transform data \n",
    "        if len(data.shape) == 1: \n",
    "            scaled_data = self.scaler.fit_transform(data.reshape(-1, 1)).flatten()\n",
    "        else: \n",
    "            scaled_data = self.scaler.fit_transform(data)\n",
    "        \n",
    "        # Return tensor of scaled data\n",
    "        return torch.tensor(scaled_data, dtype=torch.float)\n",
    "    \n",
    "    def inverse_transform(self, data): \n",
    "        # Check if data is tensor\n",
    "        if type(data) == torch.Tensor:\n",
    "            data = torch.Tensor.cpu(data).detach().numpy()\n",
    "            \n",
    "        # Check if data is dataframe \n",
    "        if type(data) == pd.DataFrame or type(data) == pd.Series:\n",
    "            data = data.values\n",
    "        \n",
    "        inverse_data = self.scaler.inverse_transform(data)\n",
    "        \n",
    "        # Return tensor of inverse data\n",
    "        return torch.tensor(inverse_data, dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Extracts m sequential data to use to predict n next data \n",
    "\"\"\"\n",
    "def extract_data(data, num_input, num_output):\n",
    "    num_rows = data.shape[0] - num_input - num_output\n",
    "    input_data = torch.zeros(num_rows, num_input)\n",
    "    output_data = torch.zeros(num_rows, num_output)\n",
    "    \n",
    "    for i in range(num_rows):\n",
    "        input_data[i] = (data[i:i+num_input])\n",
    "        output_data[i] = (data[i+num_input:i+num_input+num_output])\n",
    "    return input_data, output_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Separates data into train, validation, and test sets\n",
    "props: (train, valid)\n",
    "\"\"\"\n",
    "def train_valid_test_split(X, y=None, props=None, device=None):\n",
    "    if not props:\n",
    "        props = (0.5, 0.25)\n",
    "    elif len(props) != 3:\n",
    "        print(\"Wrong number of parameters\")\n",
    "        return None\n",
    "\n",
    "    train_size = int(X.shape[0] * props[0])\n",
    "    valid_size = int(X.shape[0] * props[1])\n",
    "\n",
    "    X_train = X[:train_size].to(device)\n",
    "    X_valid = X[train_size: (train_size + valid_size)].to(device)\n",
    "    X_test = X[(train_size + valid_size):].to(device)\n",
    "\n",
    "    if y != None:\n",
    "        y_train = y[:train_size].to(device)\n",
    "        y_valid = y[train_size: (train_size + valid_size)].to(device)\n",
    "        y_test = y[(train_size + valid_size):].to(device)\n",
    "\n",
    "        return X_train, y_train, X_valid, y_valid, X_test, y_test\n",
    "\n",
    "    return X_train, X_valid, X_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TreNet LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_layers, output_dim, device=None):\n",
    "        super(LSTM, self).__init__()\n",
    "\n",
    "        # Initialize hidden dimenision and layers\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.input_dim = input_dim\n",
    "\n",
    "        # Initialize deep learning models\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, batch_first=True).to(device)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim).to(device)\n",
    "\n",
    "        self.device = device\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_dim).requires_grad_().to(self.device)\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_dim).requires_grad_().to(self.device)\n",
    "\n",
    "        # Reshape data if needed\n",
    "        if len(x.shape) != 3:\n",
    "            x = x.reshape(x.shape[0], -1, self.input_dim).to(self.device)\n",
    "\n",
    "        # Run data through model\n",
    "        out, (hn, cn) = self.lstm(x, (h0, c0))\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TreNet LSTM Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Create training loop\n",
    "\"\"\"\n",
    "def train_loop(n_epochs, X, y, model, loss_fn, optimizer, X_val=None, y_val=None, printout=False, record_loss=False):\n",
    "    train_losses = []\n",
    "    best_model = None\n",
    "    best_loss = float(\"inf\")\n",
    "    validation = False\n",
    "    \n",
    "    # Add data for validation loss\n",
    "    if X_val is not None and y_val is not None: \n",
    "        validation = True\n",
    "        val_losses = []\n",
    "    \n",
    "    for i in range(n_epochs):\n",
    "        # Compute prediction and loss\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Compute validation loss \n",
    "        if validation:\n",
    "            val_pred = model(X_val)\n",
    "            val_loss = loss_fn(val_pred, y_val)\n",
    "            \n",
    "        # Store best performing model\n",
    "        if validation: \n",
    "            if val_loss.item() < best_loss:\n",
    "                best_loss = val_loss.item()\n",
    "                best_model = model.state_dict()\n",
    "                torch.save(model.state_dict(), \"best_model\")\n",
    "        else:\n",
    "            if loss.item() < best_loss: \n",
    "                best_loss = loss.item()\n",
    "                best_model = model.state_dict()\n",
    "                torch.save(model.state_dict(), \"best_model\")\n",
    "                \n",
    "\n",
    "        # Perform backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Print loss per epoch\n",
    "        if printout and i % 100 == 0:\n",
    "            print(f\"Epoch {i}:\\n--------------\")\n",
    "            print(f\"Train Loss: {np.sqrt(loss.item())}\")\n",
    "            \n",
    "            if val_loss is not None:\n",
    "                print(f\"Validation Loss: {np.sqrt(val_loss.item())}\")\n",
    "\n",
    "            print()\n",
    "\n",
    "        # Record loss per epoch\n",
    "        if record_loss:\n",
    "            train_losses.append(np.sqrt(loss.item()))\n",
    "            if validation:\n",
    "                val_losses.append(np.sqrt(val_loss.item()))\n",
    "    \n",
    "    # Calculate final model losses \n",
    "    pred = model(X)\n",
    "    final_train_loss = loss_fn(pred, y)\n",
    "    \n",
    "    if validation:\n",
    "        val_pred = model(X_val)\n",
    "        final_val_loss = loss_fn(val_pred, y_val)\n",
    "    \n",
    "    # Update model to best performing model\n",
    "    model.load_state_dict(torch.load(\"best_model\"))\n",
    "    pred = model(X)\n",
    "    best_train_loss = loss_fn(pred, y)\n",
    "    \n",
    "    if validation:\n",
    "        val_pred = model(X_val)\n",
    "        best_val_loss = loss_fn(val_pred, y_val)\n",
    "\n",
    "    # Print final loss after training and best performing model \n",
    "    if printout:\n",
    "        print(f\"Final:\\n--------------\")\n",
    "        print(f\"Train Loss: {np.sqrt(final_train_loss.item())}\")\n",
    "        if validation:\n",
    "            print(f\"Validation Loss: {np.sqrt(final_val_loss.item())}\")\n",
    "        print()\n",
    "        \n",
    "        print(f\"Best Model:\\n--------------\")\n",
    "        print(f\"Train Loss: {np.sqrt(best_train_loss.item())}\")\n",
    "        if validation:\n",
    "            print(f\"Validation Loss: {np.sqrt(best_val_loss.item())}\")\n",
    "        print()\n",
    "\n",
    "    if record_loss: \n",
    "        if validation:\n",
    "            return train_losses, val_losses\n",
    "        else:\n",
    "            return train_losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test LSTM Model on Raw Stock Prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in NYSE data\n",
    "df = pd.read_csv(\"../data/raw/indexProcessed.csv\")\n",
    "NYSE_df = df[df[\"Index\"] == \"NYA\"].loc[:, \"Open\"].reset_index(drop=True)\n",
    "\n",
    "# Create subset of data \n",
    "NYSE_sub = NYSE_df[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale data \n",
    "scaler = Scaler()\n",
    "sub = scaler.fit_transform(NYSE_sub).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract samples and create train test sets\n",
    "X, y = extract_data(sub, 49, 1)\n",
    "X_train, y_train, X_valid, y_valid, X_test, y_test = train_valid_test_split(X, y, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set parameters for model and training \n",
    "input_dim = 1\n",
    "hidden_dim = 32\n",
    "num_layers = 1\n",
    "output_dim = 1\n",
    "\n",
    "learning_rate = 0.01\n",
    "num_epochs = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model, loss function, and optimizer\n",
    "model = LSTM(input_dim=input_dim, hidden_dim=hidden_dim, output_dim=output_dim, num_layers=num_layers, device=device)\n",
    "loss_fn = nn.MSELoss().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:\n",
      "--------------\n",
      "Train Loss: 0.4054621613248329\n",
      "Validation Loss: 0.701968831951709\n",
      "\n",
      "Epoch 100:\n",
      "--------------\n",
      "Train Loss: 0.02297416305335471\n",
      "Validation Loss: 0.024583437615773834\n",
      "\n",
      "Epoch 200:\n",
      "--------------\n",
      "Train Loss: 0.020452765399284804\n",
      "Validation Loss: 0.0238516288571945\n",
      "\n",
      "Epoch 300:\n",
      "--------------\n",
      "Train Loss: 0.01857707044474476\n",
      "Validation Loss: 0.020738318816857112\n",
      "\n",
      "Epoch 400:\n",
      "--------------\n",
      "Train Loss: 0.01743390605534811\n",
      "Validation Loss: 0.019369173355080656\n",
      "\n",
      "Epoch 500:\n",
      "--------------\n",
      "Train Loss: 0.016884484432144888\n",
      "Validation Loss: 0.018615638232430554\n",
      "\n",
      "Epoch 600:\n",
      "--------------\n",
      "Train Loss: 0.016428816551191466\n",
      "Validation Loss: 0.01845795030551108\n",
      "\n",
      "Epoch 700:\n",
      "--------------\n",
      "Train Loss: 0.015991786953692785\n",
      "Validation Loss: 0.018535335701193336\n",
      "\n",
      "Epoch 800:\n",
      "--------------\n",
      "Train Loss: 0.015531389422403733\n",
      "Validation Loss: 0.018665742118841786\n",
      "\n",
      "Epoch 900:\n",
      "--------------\n",
      "Train Loss: 0.015310397610494568\n",
      "Validation Loss: 0.018712245349431004\n",
      "\n",
      "Final:\n",
      "--------------\n",
      "Train Loss: 0.014903782026602592\n",
      "Validation Loss: 0.018676993892149992\n",
      "\n",
      "Best Model:\n",
      "--------------\n",
      "Train Loss: 0.016317783910008828\n",
      "Validation Loss: 0.015647359957211743\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Train model\n",
    "losses = train_loop(num_epochs, X_train, y_train, model, loss_fn, optimizer, printout=True, record_loss=True, X_val = X_valid, y_val = y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare predictions and actual values \n",
    "inverse_test_y = scaler.inverse_transform(y_test)\n",
    "\n",
    "pred_test_y = model(X_test)\n",
    "inverse_pred_test_y = scaler.inverse_transform(pred_test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Test Loss: \" + str(loss_fn(inverse_pred_test_y, inverse_test_y)**(1/2)))\n",
    "\n",
    "print(\"Actual values: \\n\" + str(inverse_test_y[:10]))\n",
    "\n",
    "print(\"Predicted values: \\n\" + str(inverse_pred_test_y[:10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save training loss as csv \n",
    "pd.DataFrame(training_loss).to_csv(\"../data/losses/lstm_loss.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving Model and Restoring Model State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = model.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2 = LSTM(input_dim=input_dim, hidden_dim=hidden_dim, output_dim=output_dim, num_layers=num_layers, device=device)\n",
    "# model_2.load_state_dict(best_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('lstm.weight_ih_l0',\n",
       "              tensor([[-0.2722],\n",
       "                      [-0.1052],\n",
       "                      [ 0.2542],\n",
       "                      [ 0.2783],\n",
       "                      [-0.3370],\n",
       "                      [ 0.2374],\n",
       "                      [ 0.0372],\n",
       "                      [ 0.2592],\n",
       "                      [ 0.3185],\n",
       "                      [-0.0600],\n",
       "                      [ 0.3775],\n",
       "                      [ 0.3695],\n",
       "                      [ 0.1134],\n",
       "                      [ 0.1399],\n",
       "                      [ 0.1551],\n",
       "                      [-0.4466],\n",
       "                      [ 0.3496],\n",
       "                      [-0.0355],\n",
       "                      [ 0.0471],\n",
       "                      [-0.1455],\n",
       "                      [-0.2234],\n",
       "                      [ 0.2945],\n",
       "                      [-0.3681],\n",
       "                      [ 0.2337],\n",
       "                      [ 0.5413],\n",
       "                      [ 0.2862],\n",
       "                      [ 0.2962],\n",
       "                      [ 0.3796],\n",
       "                      [ 0.4161],\n",
       "                      [-0.0495],\n",
       "                      [ 0.1954],\n",
       "                      [ 0.3669],\n",
       "                      [-0.4020],\n",
       "                      [-0.2772],\n",
       "                      [ 0.1813],\n",
       "                      [ 0.3479],\n",
       "                      [-0.2020],\n",
       "                      [ 0.2368],\n",
       "                      [ 0.0631],\n",
       "                      [ 0.1362],\n",
       "                      [-0.4557],\n",
       "                      [-0.0512],\n",
       "                      [ 0.3890],\n",
       "                      [ 0.3326],\n",
       "                      [ 0.0602],\n",
       "                      [ 0.0358],\n",
       "                      [ 0.2758],\n",
       "                      [-0.4289],\n",
       "                      [ 0.2217],\n",
       "                      [-0.2878],\n",
       "                      [-0.0419],\n",
       "                      [-0.0554],\n",
       "                      [-0.2809],\n",
       "                      [ 0.1701],\n",
       "                      [-0.2305],\n",
       "                      [ 0.0964],\n",
       "                      [ 0.5714],\n",
       "                      [ 0.1308],\n",
       "                      [ 0.2334],\n",
       "                      [ 0.0993],\n",
       "                      [ 0.2277],\n",
       "                      [-0.0480],\n",
       "                      [ 0.2364],\n",
       "                      [ 0.3824],\n",
       "                      [ 0.3413],\n",
       "                      [-0.0914],\n",
       "                      [-0.3860],\n",
       "                      [-0.2937],\n",
       "                      [ 0.1324],\n",
       "                      [-0.0303],\n",
       "                      [-0.3979],\n",
       "                      [-0.1407],\n",
       "                      [ 0.7012],\n",
       "                      [-0.0916],\n",
       "                      [ 0.3825],\n",
       "                      [ 0.2829],\n",
       "                      [-0.0129],\n",
       "                      [ 0.1455],\n",
       "                      [ 0.4410],\n",
       "                      [-0.3479],\n",
       "                      [-0.4592],\n",
       "                      [ 0.5433],\n",
       "                      [-0.3271],\n",
       "                      [ 0.1385],\n",
       "                      [ 0.0479],\n",
       "                      [ 0.2592],\n",
       "                      [ 0.3977],\n",
       "                      [ 0.1835],\n",
       "                      [-0.4866],\n",
       "                      [-0.2944],\n",
       "                      [-0.4310],\n",
       "                      [ 0.1696],\n",
       "                      [ 0.4499],\n",
       "                      [ 0.0157],\n",
       "                      [ 0.1967],\n",
       "                      [ 0.5477],\n",
       "                      [-0.5765],\n",
       "                      [-0.2157],\n",
       "                      [ 0.6563],\n",
       "                      [ 0.2140],\n",
       "                      [-0.2706],\n",
       "                      [ 0.2118],\n",
       "                      [ 0.2012],\n",
       "                      [ 0.2059],\n",
       "                      [ 0.0397],\n",
       "                      [-0.2747],\n",
       "                      [ 0.5376],\n",
       "                      [ 0.2267],\n",
       "                      [-0.1361],\n",
       "                      [ 0.1276],\n",
       "                      [ 0.3721],\n",
       "                      [-0.7948],\n",
       "                      [ 0.4757],\n",
       "                      [-0.0471],\n",
       "                      [ 0.2800],\n",
       "                      [-0.2608],\n",
       "                      [-0.1689],\n",
       "                      [ 0.1581],\n",
       "                      [-0.4575],\n",
       "                      [ 0.3035],\n",
       "                      [ 0.6457],\n",
       "                      [ 0.1982],\n",
       "                      [ 0.4356],\n",
       "                      [ 0.4188],\n",
       "                      [ 0.4404],\n",
       "                      [-0.0657],\n",
       "                      [ 0.4846],\n",
       "                      [ 0.2249]], device='cuda:0')),\n",
       "             ('lstm.weight_hh_l0',\n",
       "              tensor([[ 0.0381,  0.1592, -0.0486,  ...,  0.1288, -0.0044, -0.0459],\n",
       "                      [ 0.0161, -0.0535,  0.1150,  ...,  0.2018, -0.0131,  0.0574],\n",
       "                      [-0.7477, -0.0447, -0.1208,  ...,  0.3627,  0.2286,  0.0814],\n",
       "                      ...,\n",
       "                      [ 0.0532,  0.0857,  0.1807,  ...,  0.2227, -0.1880, -0.1527],\n",
       "                      [ 0.0949, -0.2162,  0.1692,  ..., -0.1970, -0.0992, -0.0398],\n",
       "                      [-0.2852, -0.0324,  0.0905,  ...,  0.0669, -0.0332, -0.0792]],\n",
       "                     device='cuda:0')),\n",
       "             ('lstm.bias_ih_l0',\n",
       "              tensor([-0.0468, -0.0963,  0.2296, -0.0010,  0.0908,  0.0039, -0.0597, -0.0603,\n",
       "                       0.6802, -0.0275,  0.1428,  0.2071, -0.2335, -0.1059,  0.1520,  0.1847,\n",
       "                       0.0464,  0.5414, -0.0727, -0.1628, -0.2465, -0.0026,  0.0168, -0.1624,\n",
       "                       0.1329,  0.0564, -0.0112, -0.0294,  0.1913, -0.1763, -0.0641,  0.2246,\n",
       "                       0.1120,  0.0214, -0.1635,  0.1763, -0.0483, -0.0744,  0.1454, -0.0780,\n",
       "                      -0.2633, -0.0964, -0.1513,  0.0554, -0.1678,  0.0715, -0.1625, -0.0037,\n",
       "                       0.0104, -0.3558,  0.1813,  0.0957, -0.1087,  0.0743,  0.0502, -0.0114,\n",
       "                       0.0745, -0.1276, -0.1806, -0.0028, -0.1693, -0.0161, -0.1414,  0.0961,\n",
       "                       0.0036,  0.1478, -0.0367, -0.1055, -0.1672,  0.1438, -0.0895, -0.1267,\n",
       "                      -0.0966,  0.1063,  0.1276,  0.1711,  0.0213,  0.1306,  0.0827,  0.1865,\n",
       "                       0.1199,  0.0846, -0.0829, -0.1538, -0.1362,  0.0904,  0.0142,  0.1922,\n",
       "                      -0.0790, -0.0440, -0.1623,  0.0999,  0.1061, -0.0397,  0.2004,  0.0234,\n",
       "                      -0.0494,  0.0266,  0.0672,  0.0568,  0.1060, -0.1857,  0.0453,  0.0695,\n",
       "                       0.4281, -0.0520,  0.0064, -0.0146, -0.2385,  0.0643,  0.0940,  0.0401,\n",
       "                       0.2582,  0.2957,  0.0495, -0.2382, -0.0431,  0.1551, -0.0108,  0.0897,\n",
       "                       0.1662,  0.2121,  0.2761, -0.0880,  0.1281, -0.1636,  0.0669,  0.1046],\n",
       "                     device='cuda:0')),\n",
       "             ('lstm.bias_hh_l0',\n",
       "              tensor([ 0.0954,  0.0915,  0.3198, -0.0645, -0.1381, -0.0688,  0.0720,  0.1004,\n",
       "                       0.4597, -0.0126,  0.1898, -0.0431,  0.0195, -0.1575,  0.1426, -0.0189,\n",
       "                       0.2027,  0.3340, -0.0340, -0.1331, -0.0944, -0.0845, -0.0345,  0.1746,\n",
       "                       0.0816, -0.0261,  0.1620,  0.1506,  0.1806, -0.0066, -0.0282,  0.0970,\n",
       "                      -0.0750, -0.1753, -0.2200, -0.0881,  0.0826, -0.0277, -0.1316,  0.0041,\n",
       "                      -0.2214, -0.2445,  0.0764,  0.1567, -0.0233,  0.0470, -0.0091, -0.1665,\n",
       "                      -0.0913, -0.2592,  0.0363, -0.2474, -0.0099, -0.0558, -0.2479,  0.0167,\n",
       "                       0.0925, -0.0762, -0.0540, -0.0525,  0.1171, -0.0013, -0.0510, -0.1572,\n",
       "                      -0.1019, -0.0966,  0.0539, -0.1726,  0.0980,  0.0980, -0.1269, -0.1842,\n",
       "                       0.0555, -0.0655, -0.0013,  0.0340,  0.1636,  0.1695,  0.0235,  0.1175,\n",
       "                      -0.1036, -0.1505, -0.0039,  0.0799,  0.0988,  0.0909, -0.1304, -0.0995,\n",
       "                      -0.1199, -0.1764,  0.1464,  0.1731, -0.0412, -0.0557,  0.1646, -0.0385,\n",
       "                       0.1424, -0.0736,  0.0107,  0.0383,  0.1048, -0.1033,  0.0732, -0.1062,\n",
       "                       0.6442, -0.2236,  0.1676, -0.1386, -0.0923, -0.1331, -0.0089, -0.0337,\n",
       "                       0.0255,  0.3823, -0.0445, -0.1639, -0.1740,  0.1364,  0.0171,  0.0267,\n",
       "                       0.0095,  0.0958,  0.2780,  0.0175,  0.1653, -0.1634,  0.0175,  0.1541],\n",
       "                     device='cuda:0')),\n",
       "             ('fc.weight',\n",
       "              tensor([[ 0.1585, -0.0314, -0.2133, -0.1879,  0.0380,  0.0833, -0.1557, -0.0758,\n",
       "                        0.6377,  0.0131,  0.2634,  0.1878,  0.0733,  0.1991,  0.1657, -0.2023,\n",
       "                       -0.3022,  0.3986, -0.2297,  0.0094,  0.0469,  0.2611,  0.2246,  0.1155,\n",
       "                       -0.1584, -0.2497, -0.2236,  0.1913,  0.3231,  0.0606,  0.2345,  0.2851]],\n",
       "                     device='cuda:0')),\n",
       "             ('fc.bias', tensor([-0.0492], device='cuda:0'))])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_2.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_2.load_state_dict(dict(model.state_dict()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
