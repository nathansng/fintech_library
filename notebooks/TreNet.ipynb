{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5312ddb9",
   "metadata": {},
   "source": [
    "# TreNet Implementation\n",
    "\n",
    "### Authors: Nathan Ng"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d6be7a2",
   "metadata": {},
   "source": [
    "### TreNet Implementation\n",
    "\n",
    "- Feeds trend duration and slope into LSTM \n",
    "- Feeds corresponding data points with trends to CNN stack \n",
    "- Combines output of LSTM and CNN in feature fusion layer\n",
    "- Outputs final prediction with fully connected layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "74c9ebe9-8c3c-4504-81e2-7f78863cb53f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import ast\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0574aec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use GPU if available for PyTorch\n",
    "if torch.cuda.is_available():\n",
    "    dev = \"cuda:0\"\n",
    "else:\n",
    "    dev = \"cpu\"\n",
    "\n",
    "device = torch.device(dev)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88aac5c9",
   "metadata": {},
   "source": [
    "## Data Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "037f5d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_data(data):\n",
    "    \"\"\"\n",
    "    Pad all rows with 0's to match longest row \n",
    "    \"\"\"\n",
    "    max_len = data.apply(len).max()\n",
    "    \n",
    "    pad_row = lambda x: ([0] * (max_len - len(x))) + x\n",
    "    padded_data = data.apply(pad_row)\n",
    "    return padded_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4f68dabe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_data_points(data):\n",
    "    \"\"\"\n",
    "    Takes in dataframe of [duration, slope, data_points]\n",
    "    Returns tensor of trend data and tensor of corresponding data points\n",
    "    \"\"\"\n",
    "    \n",
    "    # Extract trends data \n",
    "    trends = data[['duration', 'slope']]\n",
    "    trends = torch.tensor(trends.values)\n",
    "    \n",
    "    # Extract data points \n",
    "    data_pts = data['data_points'].apply(ast.literal_eval)\n",
    "    data_pts = pad_data(data_pts)\n",
    "    data_pts = torch.tensor(data_pts)\n",
    "    \n",
    "    return trends, data_pts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ec5dfd29",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Scales data using Sklearn's MinMaxScaler\n",
    "Generalized to accept tensors \n",
    "\"\"\"\n",
    "class Scaler():\n",
    "    def __init__(self):\n",
    "        self.scaler = MinMaxScaler()\n",
    "        \n",
    "    def fit_transform(self, data):\n",
    "        # Check if data is tensor\n",
    "        if type(data) == torch.Tensor:\n",
    "            data = torch.Tensor.cpu(data).detach().numpy()\n",
    "            \n",
    "        # Check if data is dataframe \n",
    "        if type(data) == pd.DataFrame or type(data) == pd.Series:\n",
    "            data = data.values\n",
    "            \n",
    "        # Transform data \n",
    "        if len(data.shape) == 1: \n",
    "            scaled_data = self.scaler.fit_transform(data.reshape(-1, 1)).flatten()\n",
    "        else: \n",
    "            scaled_data = self.scaler.fit_transform(data)\n",
    "        \n",
    "        # Return tensor of scaled data\n",
    "        return torch.tensor(scaled_data, dtype=torch.float)\n",
    "    \n",
    "    def inverse_transform(self, data): \n",
    "        # Check if data is tensor\n",
    "        if type(data) == torch.Tensor:\n",
    "            data = torch.Tensor.cpu(data).detach().numpy()\n",
    "            \n",
    "        # Check if data is dataframe \n",
    "        if type(data) == pd.DataFrame or type(data) == pd.Series:\n",
    "            data = data.values\n",
    "        \n",
    "        inverse_data = self.scaler.inverse_transform(data)\n",
    "        \n",
    "        # Return tensor of inverse data\n",
    "        return torch.tensor(inverse_data, dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "30cf49c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Extracts m sequential data to use to predict n next data \n",
    "\"\"\"\n",
    "def extract_data(data, num_input, num_output):\n",
    "    num_rows = data.shape[0] - num_input - num_output\n",
    "    if len(data.shape) == 2: \n",
    "        input_data = torch.zeros(num_rows, num_input, data.shape[-1])\n",
    "        output_data = torch.zeros(num_rows, num_output, data.shape[-1])\n",
    "    else: \n",
    "        input_data = torch.zeros(num_rows, num_input)\n",
    "        output_data = torch.zeros(num_rows, num_output)\n",
    "    \n",
    "    for i in range(num_rows):\n",
    "        input_data[i] = (data[i:i+num_input])\n",
    "        output_data[i] = (data[i+num_input:i+num_input+num_output])\n",
    "    return input_data, output_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "426641b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Separates data into train, validation, and test sets\n",
    "props: (train, valid)\n",
    "\"\"\"\n",
    "def train_valid_test_split(X, y=None, props=None):\n",
    "    if not props: \n",
    "        props = (0.5, 0.25)\n",
    "    elif len(props) != 3: \n",
    "        print(\"Wrong number of parameters\")\n",
    "        return None\n",
    "    \n",
    "    train_size = int(X.shape[0] * props[0])\n",
    "    valid_size = int(X.shape[0] * props[1])\n",
    "    \n",
    "    X_train = X[:train_size].to(device)\n",
    "    X_valid = X[train_size: (train_size + valid_size)].to(device)\n",
    "    X_test = X[(train_size + valid_size):].to(device)\n",
    "    \n",
    "    if y != None: \n",
    "        y_train = y[:train_size].to(device)\n",
    "        y_valid = y[train_size: (train_size + valid_size)].to(device)\n",
    "        y_test = y[(train_size + valid_size):].to(device)\n",
    "        \n",
    "        return X_train, y_train, X_valid, y_valid, X_test, y_test\n",
    "    \n",
    "    return X_train, X_valid, X_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3340bb9",
   "metadata": {},
   "source": [
    "## TreNet Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9840f259",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_layers, output_dim):\n",
    "        super(LSTM, self).__init__()\n",
    "        \n",
    "        # Initialize hidden dimenision and layers \n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.input_dim = input_dim\n",
    "        \n",
    "        # Initialize deep learning models\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_dim).requires_grad_().to(device)\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_dim).requires_grad_().to(device)\n",
    "        \n",
    "        # Reshape data if needed\n",
    "        if len(x.shape) != 3: \n",
    "            x = x.reshape(x.shape[0], -1, self.input_dim)\n",
    "        \n",
    "        # Run data through model\n",
    "        out, (hn, cn) = self.lstm(x, (h0, c0))\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d4fb1c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TreNetCNN(nn.Module):\n",
    "    def __init__(self, num_data, layers=None, num_filters=None, dropout=None, conv_size=3, pooling_size=3, output_size=2):\n",
    "        \"\"\"\n",
    "        layers (int): Number of cnn stacks to create \n",
    "        num_filters (list(int)): Number of CNN filters corresponding to same index stacks\n",
    "        dropout (list(float)): Probability of dropout corresponing to same index stack\n",
    "        convsize (int, list(int)): Size of filter sizes \n",
    "        \"\"\"\n",
    "        super(TreNetCNN, self).__init__()\n",
    "        self.num_data = num_data\n",
    "        self.layers = layers\n",
    "        self.num_filters = num_filters\n",
    "        self.dropout = dropout\n",
    "        self.conv_size = conv_size\n",
    "        self.pooling_size = pooling_size\n",
    "        self.output_size = output_size\n",
    "        self.cnn_stack = self.create_cnn_stack()\n",
    "    \n",
    "    def create_cnn_stack(self):\n",
    "        # Initialize default stack settings\n",
    "        if not self.layers:\n",
    "            self.layers = 2\n",
    "        if not self.num_filters:\n",
    "            self.num_filters = [128] * self.layers\n",
    "        if not self.dropout:\n",
    "            self.dropout = [0.0] * self.layers\n",
    "        if type(self.conv_size) == int:\n",
    "            self.conv_size = [self.conv_size] * self.layers\n",
    "        \n",
    "        # Create cnn stacks \n",
    "        cnn_stacks = []\n",
    "        num_channels = 1\n",
    "        updated_data = self.num_data\n",
    "        for i in range(self.layers):\n",
    "            cnn_stack = nn.Sequential(\n",
    "                nn.Conv1d(in_channels=num_channels, out_channels=self.num_filters[i], kernel_size=self.conv_size[i]),\n",
    "                nn.ReLU(),\n",
    "                nn.MaxPool1d(kernel_size=self.pooling_size, stride=1),\n",
    "                nn.Dropout(p=self.dropout[i])\n",
    "            )\n",
    "            num_channels = self.num_filters[i]\n",
    "            cnn_stacks.append(cnn_stack)\n",
    "            \n",
    "            # Keep track of current size of data \n",
    "            updated_data = updated_data - self.conv_size[i] + 1 - self.pooling_size + 1\n",
    "            new_data_size = self.num_filters[i] * updated_data\n",
    "            \n",
    "        # Add fully connected layer at end to output trend duration and slope\n",
    "        output_layer = nn.Sequential(\n",
    "            nn.Flatten(), \n",
    "            nn.Linear(new_data_size, self.output_size)\n",
    "        )\n",
    "        cnn_stacks.append(output_layer)\n",
    "            \n",
    "        # Combine cnn stacks \n",
    "        return nn.Sequential(*cnn_stacks)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = torch.reshape(x, (x.shape[0], 1, -1))\n",
    "        output = self.cnn_stack(x)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a35fa6b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TreNet(nn.Module):\n",
    "    def __init__(self, LSTM_params, CNN_params, feature_fusion, output_dim):\n",
    "        super(TreNet, self).__init__()\n",
    "        \n",
    "        # Set number of parameters for feature fusion layer\n",
    "        LSTM_params['output_dim'] = feature_fusion\n",
    "        CNN_params['output_size'] = feature_fusion\n",
    "        \n",
    "        self.lstm = LSTM(**LSTM_params)\n",
    "        self.cnn = TreNetCNN(**CNN_params)\n",
    "        self.fusion = nn.Linear(feature_fusion, output_dim)\n",
    "        self.cutoff = CNN_params['num_data']\n",
    "        \n",
    "    def forward(self, data):\n",
    "        trends, data = data\n",
    "        \n",
    "        # Run trends through LSTM \n",
    "        lstm_out = self.lstm(trends)\n",
    "        \n",
    "        # Set cutoff for CNN stock prices\n",
    "        cutoff_data = torch.zeros(data.shape[0], self.cutoff).to(device)\n",
    "        for i in range(data.shape[0]):\n",
    "            cutoff_data[i] = data[i, -self.cutoff:]\n",
    "        \n",
    "        # Run stock prices through CNN\n",
    "        cnn_out = self.cnn(cutoff_data)\n",
    "        \n",
    "        # Concat outputs in feature fusion layer \n",
    "        feature_in = torch.add(lstm_out, cnn_out)\n",
    "        \n",
    "        # Run outputs through feature fusion layer \n",
    "        output = self.fusion(feature_in)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bdc2eb2",
   "metadata": {},
   "source": [
    "## TreNet Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5a4f5282",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Create training loop\n",
    "\"\"\"\n",
    "def train_loop(n_epochs, X, y, model, loss_fn, optimizer, printout=False, record_loss=False):\n",
    "    losses = []\n",
    "    for i in range(n_epochs):\n",
    "        # Compute prediction and loss \n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "        \n",
    "        # Perform backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Print loss per epoch \n",
    "        if printout and i % 100 == 0: \n",
    "            print(f\"Epoch {i}:\\n--------------\")\n",
    "            print(f\"Train Loss: {np.sqrt(loss.item())}\")\n",
    "            print()\n",
    "            \n",
    "        # Record loss per epoch \n",
    "        if record_loss: \n",
    "            losses.append(np.sqrt(loss.item()))\n",
    "            \n",
    "    # Print final loss after training \n",
    "    if printout:\n",
    "        print(f\"Final:\\n--------------\")\n",
    "        print(f\"Train Loss: {np.sqrt(loss.item())}\")\n",
    "        print()\n",
    "    \n",
    "    if record_loss:\n",
    "        return losses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a6f5df8",
   "metadata": {},
   "source": [
    "## Test TreNet Model on Trend and Stock Price Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f541a60d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in trends dataset \n",
    "df = pd.read_csv(\"../data/processed/processed_trends_10.csv\")\n",
    "trends, data = convert_data_points(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4d3b9715",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale data \n",
    "trend_scaler = Scaler()\n",
    "data_scaler = Scaler()\n",
    "\n",
    "scaled_trends = trend_scaler.fit_transform(trends)\n",
    "scaled_data = data_scaler.fit_transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f638445b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract samples and create train test sets \n",
    "trend_X, trend_y = extract_data(scaled_trends, 50, 1)\n",
    "data_X = scaled_data[51:]\n",
    "\n",
    "X_train_trend, y_train_trend, X_valid_trend, y_valid_trend, X_test_trend, y_test_trend = train_valid_test_split(trend_X, trend_y)\n",
    "X_train_data, X_valid_data, X_test_data = train_valid_test_split(data_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4415249b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set parameters for model and training \n",
    "LSTM_params = {\n",
    "    'input_dim': 2, \n",
    "    'hidden_dim': 32, \n",
    "    'num_layers': 1\n",
    "}\n",
    "\n",
    "CNN_params = {\n",
    "    'num_data': 10,\n",
    "    'layers': 2, \n",
    "    'num_filters': [32, 32],\n",
    "    'dropout': [0.1, 0.1], \n",
    "    'conv_size': [2, 4]\n",
    "}\n",
    "\n",
    "learning_rate = 0.01\n",
    "num_epochs = 4000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a890c335",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model, loss function, and optimizer\n",
    "model = TreNet(LSTM_params, CNN_params, 1, 2).to(device)\n",
    "loss_fn = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4347fe82",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/torch/nn/functional.py:652: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)\n",
      "  return torch.max_pool1d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:\n",
      "--------------\n",
      "Train Loss: 0.7039901920442765\n",
      "\n",
      "Epoch 100:\n",
      "--------------\n",
      "Train Loss: 0.12079283411379761\n",
      "\n",
      "Epoch 200:\n",
      "--------------\n",
      "Train Loss: 0.1181159709767101\n",
      "\n",
      "Epoch 300:\n",
      "--------------\n",
      "Train Loss: 0.11708236587736737\n",
      "\n",
      "Epoch 400:\n",
      "--------------\n",
      "Train Loss: 0.11712215509084233\n",
      "\n",
      "Epoch 500:\n",
      "--------------\n",
      "Train Loss: 0.11677338226739689\n",
      "\n",
      "Epoch 600:\n",
      "--------------\n",
      "Train Loss: 0.116406884607723\n",
      "\n",
      "Epoch 700:\n",
      "--------------\n",
      "Train Loss: 0.11638243626863232\n",
      "\n",
      "Epoch 800:\n",
      "--------------\n",
      "Train Loss: 0.11589680215718749\n",
      "\n",
      "Epoch 900:\n",
      "--------------\n",
      "Train Loss: 0.1151638340425978\n",
      "\n",
      "Epoch 1000:\n",
      "--------------\n",
      "Train Loss: 0.11470266291300915\n",
      "\n",
      "Epoch 1100:\n",
      "--------------\n",
      "Train Loss: 0.11301871823677098\n",
      "\n",
      "Epoch 1200:\n",
      "--------------\n",
      "Train Loss: 0.11157638909464349\n",
      "\n",
      "Epoch 1300:\n",
      "--------------\n",
      "Train Loss: 0.11009741947053193\n",
      "\n",
      "Epoch 1400:\n",
      "--------------\n",
      "Train Loss: 0.11183010667050051\n",
      "\n",
      "Epoch 1500:\n",
      "--------------\n",
      "Train Loss: 0.11017417998520528\n",
      "\n",
      "Epoch 1600:\n",
      "--------------\n",
      "Train Loss: 0.10883743299727756\n",
      "\n",
      "Epoch 1700:\n",
      "--------------\n",
      "Train Loss: 0.10823626458336036\n",
      "\n",
      "Epoch 1800:\n",
      "--------------\n",
      "Train Loss: 0.10751675403829851\n",
      "\n",
      "Epoch 1900:\n",
      "--------------\n",
      "Train Loss: 0.10555697212798908\n",
      "\n",
      "Epoch 2000:\n",
      "--------------\n",
      "Train Loss: 0.10393604339019365\n",
      "\n",
      "Epoch 2100:\n",
      "--------------\n",
      "Train Loss: 0.10733857826756608\n",
      "\n",
      "Epoch 2200:\n",
      "--------------\n",
      "Train Loss: 0.10430994809116491\n",
      "\n",
      "Epoch 2300:\n",
      "--------------\n",
      "Train Loss: 0.10369820956704992\n",
      "\n",
      "Epoch 2400:\n",
      "--------------\n",
      "Train Loss: 0.09723663779042092\n",
      "\n",
      "Epoch 2500:\n",
      "--------------\n",
      "Train Loss: 0.09585732571308624\n",
      "\n",
      "Epoch 2600:\n",
      "--------------\n",
      "Train Loss: 0.08972330935289212\n",
      "\n",
      "Epoch 2700:\n",
      "--------------\n",
      "Train Loss: 0.0860917754820913\n",
      "\n",
      "Epoch 2800:\n",
      "--------------\n",
      "Train Loss: 0.0792655892502704\n",
      "\n",
      "Epoch 2900:\n",
      "--------------\n",
      "Train Loss: 0.07402256396440003\n",
      "\n",
      "Epoch 3000:\n",
      "--------------\n",
      "Train Loss: 0.07028250584343262\n",
      "\n",
      "Epoch 3100:\n",
      "--------------\n",
      "Train Loss: 0.06491098551884193\n",
      "\n",
      "Epoch 3200:\n",
      "--------------\n",
      "Train Loss: 0.05635845256094956\n",
      "\n",
      "Epoch 3300:\n",
      "--------------\n",
      "Train Loss: 0.050357161914169384\n",
      "\n",
      "Epoch 3400:\n",
      "--------------\n",
      "Train Loss: 0.04178149015939662\n",
      "\n",
      "Epoch 3500:\n",
      "--------------\n",
      "Train Loss: 0.03787334726602137\n",
      "\n",
      "Epoch 3600:\n",
      "--------------\n",
      "Train Loss: 0.05681259259183869\n",
      "\n",
      "Epoch 3700:\n",
      "--------------\n",
      "Train Loss: 0.029458349991955672\n",
      "\n",
      "Epoch 3800:\n",
      "--------------\n",
      "Train Loss: 0.023655110915137185\n",
      "\n",
      "Epoch 3900:\n",
      "--------------\n",
      "Train Loss: 0.020628673049623177\n",
      "\n",
      "Final:\n",
      "--------------\n",
      "Train Loss: 0.02417747543289362\n",
      "\n"
     ]
    }
   ],
   "source": [
    "training_loss = train_loop(num_epochs, [X_train_trend, X_train_data], y_train_trend.reshape(y_train_trend.shape[0], 2), model, loss_fn, optimizer, printout=True, record_loss=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "33d10df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare predictions and actual values \n",
    "inverse_test_y = trend_scaler.inverse_transform(y_test_trend.reshape(y_test_trend.shape[0], y_test_trend.shape[2]))\n",
    "\n",
    "pred_test_y = model([X_test_trend, X_test_data])\n",
    "inverse_pred_test_y = trend_scaler.inverse_transform(pred_test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8823b41b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Duration Loss: tensor(7.3193)\n",
      "Test Slope Loss: tensor(20.6998)\n",
      "Actual values: \n",
      "tensor([[ 16.0000,  38.6751],\n",
      "        [ 16.0000,  15.4913],\n",
      "        [ 16.0000,  27.3118],\n",
      "        [ 16.0000,  11.4776],\n",
      "        [ 16.0000, -17.3519],\n",
      "        [ 16.0000,   0.2702],\n",
      "        [ 16.0000,  23.4986],\n",
      "        [ 22.0000,   7.4000],\n",
      "        [ 16.0000,  -5.8514],\n",
      "        [ 16.0000,  14.2896]])\n",
      "Predicted values: \n",
      "tensor([[21.2697,  0.3765],\n",
      "        [ 4.6700,  0.4781],\n",
      "        [ 5.6269,  0.4723],\n",
      "        [12.7729,  0.4285],\n",
      "        [19.2507,  0.3889],\n",
      "        [20.0615,  0.3839],\n",
      "        [ 9.5527,  0.4482],\n",
      "        [ 3.6829,  0.4842],\n",
      "        [ 5.2429,  0.4746],\n",
      "        [16.8651,  0.4035]])\n"
     ]
    }
   ],
   "source": [
    "print(\"Test Duration Loss: \" + str(loss_fn(inverse_pred_test_y[:, 0], inverse_test_y[:, 0])**(1/2)))\n",
    "print(\"Test Slope Loss: \" + str(loss_fn(inverse_pred_test_y[:, 1], inverse_test_y[:, 1])**(1/2)))\n",
    "\n",
    "print(\"Actual values: \\n\" + str(inverse_test_y[:10]))\n",
    "\n",
    "print(\"Predicted values: \\n\" + str(inverse_pred_test_y[:10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e112e144",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save training loss as csv \n",
    "pd.DataFrame(training_loss).to_csv(\"../data/losses/trenet_loss.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
